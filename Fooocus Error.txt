D:\YouTube\Fooocus\Fooocus_win64_2-1-831>.\python_embeded\python.exe -s Fooocus\entry_with_update.py --directml
Already up-to-date
Update succeeded.
[System ARGV] ['Fooocus\\entry_with_update.py', '--directml']
Python 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]
Fooocus version: 2.1.865
Running on local URL:  http://127.0.0.1:7865

To create a public link, set `share=True` in `launch()`.
Using directml with device:
Total VRAM 1024 MB, total RAM 7538 MB
Set vram state to: NORMAL_VRAM
Always offload VRAM
Device: privateuseone
VAE dtype: torch.float32
Using sub quadratic optimization for cross attention, if you have memory or speed issues try using: --attention-split
Refiner unloaded.
model_type EPS
UNet ADM Dimension 2816
Using split attention in VAE
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
Using split attention in VAE
extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids', 'cond_stage_model.clip_l.text_projection'}
Base model loaded: D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\models\checkpoints\juggernautXL_v8Rundiffusion.safetensors
Request to load LoRAs [['sd_xl_offset_example-lora_1.0.safetensors', 0.1], ['None', 1.0], ['None', 1.0], ['None', 1.0], ['None', 1.0]] for model [D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\models\checkpoints\juggernautXL_v8Rundiffusion.safetensors].
Loaded LoRA [D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\models\loras\sd_xl_offset_example-lora_1.0.safetensors] for UNet [D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\models\checkpoints\juggernautXL_v8Rundiffusion.safetensors] with 788 keys at weight 0.1.
Fooocus V2 Expansion: Vocab with 642 words.
Fooocus Expansion engine loaded for cpu, use_fp16 = False.
Requested to load SDXLClipModel
Requested to load GPT2LMHeadModel
Loading 2 new models
App started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865
[Parameters] Adaptive CFG = 7
[Parameters] Sharpness = 2
[Parameters] ADM Scale = 1.5 : 0.8 : 0.3
[Parameters] CFG = 4.0
[Parameters] Seed = 8353953712599299144
[Parameters] Sampler = dpmpp_2m_sde_gpu - karras
[Parameters] Steps = 30 - 15
[Fooocus] Initializing ...
[Fooocus] Loading models ...
Refiner unloaded.
[Fooocus] Processing prompts ...
[Fooocus] Preparing Fooocus text #1 ...
[Prompt Expansion] Lord Ganesh, highly detailed, sharp focus, intricate, cinematic light, directed, perfect composition, beautiful dynamic dramatic bright colors, joyful atmosphere, precise, charming, depicted, vivid, gorgeous, very inspirational, original, creative, artistic, winning, color clear, relaxed, attractive, friendly, calm, cute, professional, enhanced, romantic, loving, pretty, determined
[Fooocus] Preparing Fooocus text #2 ...
[Prompt Expansion] Lord Ganesh, highly detailed, excellent composition, cinematic atmosphere, dynamic dramatic beautiful light, aesthetic, very inspirational,, illustrious, fine detail, full background, intricate, elegant, epic, designed, sharp focus, perfect complex, vibrant color, ambient, professional, heavenly, magical, awesome, symmetry, inspired, deep colors, iconic, read whole novel, rational, clear
[Fooocus] Encoding positive #1 ...
[Fooocus] Encoding positive #2 ...
[Fooocus] Encoding negative #1 ...
[Fooocus] Encoding negative #2 ...
[Parameters] Denoising Strength = 1.0
[Parameters] Initial Latent shape: Image Space (896, 1152)
Preparation time: 34.42 seconds
[Sampler] refiner_swap_method = joint
[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828
Requested to load SDXL
Loading 1 new model
loading in lowvram mode 64.0
[Fooocus Model Management] Moving model(s) has taken 74.85 seconds
  0%|                                                                                           | 0/30 [00:00<?, ?it/s][W D:\a\_work\1\s\pytorch-directml-plugin\torch_directml\csrc\engine\dml_heap_allocator.cc:120] DML allocator out of memory!
  0%|                                                                                           | 0/30 [01:08<?, ?it/s]
Traceback (most recent call last):
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\async_worker.py", line 822, in worker
    handler(task)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\async_worker.py", line 753, in handler
    imgs = pipeline.process_diffusion(
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\default_pipeline.py", line 361, in process_diffusion
    sampled_latent = core.ksampler(
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\core.py", line 313, in ksampler
    samples = ldm_patched.modules.sample.sample(model,
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\modules\sample.py", line 100, in sample
    samples = sampler.sample(noise, positive_copy, negative_copy, cfg=cfg, latent_image=latent_image, start_step=start_step, last_step=last_step, force_full_denoise=force_full_denoise, denoise_mask=noise_mask, sigmas=sigmas, callback=callback, disable_pbar=disable_pbar, seed=seed)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\modules\samplers.py", line 712, in sample
    return sample(self.model, noise, positive, negative, cfg, self.device, sampler, sigmas, self.model_options, latent_image=latent_image, denoise_mask=denoise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\sample_hijack.py", line 157, in sample_hacked
    samples = sampler.sample(model_wrap, sigmas, extra_args, callback_wrap, noise, latent_image, denoise_mask, disable_pbar)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\modules\samplers.py", line 557, in sample
    samples = self.sampler_function(model_k, noise, sigmas, extra_args=extra_args, callback=k_callback, disable=disable_pbar, **self.extra_options)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\k_diffusion\sampling.py", line 701, in sample_dpmpp_2m_sde_gpu
    return sample_dpmpp_2m_sde(model, x, sigmas, extra_args=extra_args, callback=callback, disable=disable, eta=eta, s_noise=s_noise, noise_sampler=noise_sampler, solver_type=solver_type)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\k_diffusion\sampling.py", line 613, in sample_dpmpp_2m_sde
    denoised = model(x, sigmas[i] * s_in, **extra_args)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\patch.py", line 314, in patched_KSamplerX0Inpaint_forward
    out = self.inner_model(x, sigma,
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\modules\samplers.py", line 271, in forward
    return self.apply_model(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\modules\samplers.py", line 268, in apply_model
    out = sampling_function(self.inner_model, x, timestep, uncond, cond, cond_scale, model_options=model_options, seed=seed)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\patch.py", line 229, in patched_sampling_function
    positive_x0, negative_x0 = calc_cond_uncond_batch(model, cond, uncond, x, timestep, model_options)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\modules\samplers.py", line 222, in calc_cond_uncond_batch
    output = model.apply_model(input_x, timestep_, **c).chunk(batch_chunks)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\modules\model_base.py", line 85, in apply_model
    model_output = self.diffusion_model(xc, t, context=context, control=control, transformer_options=transformer_options, **extra_conds).float()
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\patch.py", line 431, in patched_unet_forward
    h = forward_timestep_embed(module, h, emb, context, transformer_options, output_shape, time_context=time_context, num_video_frames=num_video_frames, image_only_indicator=image_only_indicator)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\diffusionmodules\openaimodel.py", line 43, in forward_timestep_embed
    x = layer(x, context, transformer_options)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\attention.py", line 613, in forward
    x = block(x, context=context[i], transformer_options=transformer_options)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\attention.py", line 440, in forward
    return checkpoint(self._forward, (x, context, transformer_options), self.parameters(), self.checkpoint)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\diffusionmodules\util.py", line 189, in checkpoint
    return func(*inputs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\attention.py", line 500, in _forward
    n = self.attn1(n, context=context_attn1, value=value_attn1)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\attention.py", line 392, in forward
    out = optimized_attention(q, k, v, self.heads)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\attention.py", line 168, in attention_sub_quad
    hidden_states = efficient_dot_product_attention(
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\sub_quadratic_attention.py", line 265, in efficient_dot_product_attention
    res = torch.cat([
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\sub_quadratic_attention.py", line 267, in <listcomp>
    query=get_query_chunk(i * query_chunk_size),
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\sub_quadratic_attention.py", line 227, in get_query_chunk
    return dynamic_slice(
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\ldm_patched\ldm\modules\sub_quadratic_attention.py", line 35, in dynamic_slice
    return x[slicing]
RuntimeError: The GPU will not respond to more commands, most likely because some other application submitted invalid commands.
The calling application should re-create the device and continue.
Total time: 249.37 seconds
[Parameters] Adaptive CFG = 7
[Parameters] Sharpness = 2
[Parameters] ADM Scale = 1.5 : 0.8 : 0.3
[Parameters] CFG = 4.0
[Parameters] Seed = 4148846922569067083
[Parameters] Sampler = dpmpp_2m_sde_gpu - karras
[Parameters] Steps = 30 - 15
[Fooocus] Initializing ...
[Fooocus] Loading models ...
Refiner unloaded.
[Fooocus] Processing prompts ...
[Fooocus] Preparing Fooocus text #1 ...
[Prompt Expansion] Lord Ganesh, extremely detailed, fine detail, intricate, elite, elegant, luxury, cinematic, dynamic light, elaborate, shining, rich deep colors, vivid color, coherent, symmetry, iconic, sublime, incredible, magnificent, full, complex, colorful, astonishing, amazing composition, background, epic, great artistic, pure, wonderful atmosphere, thought, dramatic, fantastic
[Fooocus] Preparing Fooocus text #2 ...
[Prompt Expansion] Lord Ganesh, extremely detailed, excellent composition, cinematic atmosphere, dynamic dramatic aesthetic, very inspirational, highly detail, vibrant colors, inspired, deep color, intricate, beautiful, stunning epic, breathtaking, creative, winning scenic, futuristic, thought, perfect, awesome, sunny, illuminated, shining, clear, magnificent, colossal, singular, peaceful, pure, light, cool
[Fooocus] Encoding positive #1 ...
[Fooocus] Encoding positive #2 ...
[Fooocus] Encoding negative #1 ...
[Fooocus] Encoding negative #2 ...
[Parameters] Denoising Strength = 1.0
[Parameters] Initial Latent shape: Image Space (896, 1152)
Preparation time: 44.34 seconds
[Sampler] refiner_swap_method = joint
[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828
Traceback (most recent call last):
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\async_worker.py", line 822, in worker
    handler(task)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\async_worker.py", line 753, in handler
    imgs = pipeline.process_diffusion(
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\default_pipeline.py", line 355, in process_diffusion
    initial_latent['samples'].to(ldm_patched.modules.model_management.get_torch_device()),
RuntimeError: The GPU device instance has been suspended. Use GetDeviceRemovedReason to determine the appropriate action.
Total time: 44.55 seconds
[Parameters] Adaptive CFG = 7
[Parameters] Sharpness = 2
[Parameters] ADM Scale = 1.5 : 0.8 : 0.3
[Parameters] CFG = 4.0
[Parameters] Seed = 800880626641565281
[Parameters] Sampler = dpmpp_2m_sde_gpu - karras
[Parameters] Steps = 30 - 15
[Fooocus] Initializing ...
[Fooocus] Loading models ...
Refiner unloaded.
[Fooocus] Processing prompts ...
[Fooocus] Preparing Fooocus text #1 ...
[Prompt Expansion] Lord Ganesh, highly detailed, intricate, sharp focus, dramatic cinematic full light, dynamic background, excellent composition, ambient glowing, aesthetic, very inspirational, rich deep colors, fine detail, winning color, perfect balance, calm vivid beautiful, creative, positive, lively atmosphere, novel, cute, lovely, magical, pretty, scenic, artistic, thought, clear, ideal
[Fooocus] Preparing Fooocus text #2 ...
[Prompt Expansion] Lord Ganesh, elegant intricate futuristic enchanted, highly detailed, dynamic, sharp focus, cool color light, background bright colors, inspired, heavenly, magic, new, shiny, colorful, surreal, symmetry, fine detail, polished, complex, amazing composition, creative, atmosphere, cinematic, beautiful vivid, very inspirational, thought, professional, positive, thoughtful, unique, attractive
[Fooocus] Encoding positive #1 ...
[Fooocus] Encoding positive #2 ...
[Fooocus] Encoding negative #1 ...
[Fooocus] Encoding negative #2 ...
[Parameters] Denoising Strength = 1.0
[Parameters] Initial Latent shape: Image Space (896, 1152)
Preparation time: 39.62 seconds
[Sampler] refiner_swap_method = joint
[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828
Traceback (most recent call last):
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\async_worker.py", line 822, in worker
    handler(task)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\async_worker.py", line 753, in handler
    imgs = pipeline.process_diffusion(
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\python_embeded\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "D:\YouTube\Fooocus\Fooocus_win64_2-1-831\Fooocus\modules\default_pipeline.py", line 355, in process_diffusion
    initial_latent['samples'].to(ldm_patched.modules.model_management.get_torch_device()),
RuntimeError: The GPU device instance has been suspended. Use GetDeviceRemovedReason to determine the appropriate action.
Total time: 39.80 seconds
Keyboard interruption in main thread... closing server.

D:\YouTube\Fooocus\Fooocus_win64_2-1-831>